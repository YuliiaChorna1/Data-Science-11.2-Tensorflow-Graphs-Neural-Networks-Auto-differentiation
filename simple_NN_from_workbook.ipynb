{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нехай потрібно написати примітивну нейронну мережу, яка обчислює значення виразу \n",
    "$$w * x + b$$ \n",
    "Це можна було б зробити наступним чином:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SimpleModule(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(5.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.w * x + self.b\n",
    "\n",
    "\n",
    "simple_module = SimpleModule(name=\"simple\")\n",
    "simple_module(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обов'язковою є реалізація магічного методу `__cal__`, оскільки всі фактичні обчислення відбуваються саме там."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми можемо спробувати створити складнішу нейронну мережу. Нехай потрібно створити нейромережу, в якій **2 шари**. На першому шарі має 3 нейрони з трьома входами, на другому - 1, в якості активаційної функції використовуватимемо ReLU. Реалізація такої нейромережі представлена далі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class DenseLayer(tf.Module):\n",
    "    def __init__(self, in_features, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name=\"w\")\n",
    "        self.b = tf.Variable(tf.zeros([out_features]), name=\"b\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)\n",
    "\n",
    "\n",
    "class NN(tf.Module):\n",
    "  def __init__(self, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.layer_1 = DenseLayer(in_features=3, out_features=3)\n",
    "    self.layer_2 = DenseLayer(in_features=3, out_features=1)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.layer_1(x)\n",
    "    return self.layer_2(x)\n",
    "\n",
    "\n",
    "nn = NN(name=\"neural_network\")\n",
    "print(\"Results:\", nn(tf.constant([[2.0, 2.0, 2.0]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mи створили просту нейронну мережу, що складається із двох шарів. Поки що така нейронна мережа марна. Щоб із нею можна було щось робити, її треба навчити. Навчати нейронні мережі можна по-різному і в tensorflow для цього є відповідні механізми. Для демонстрації нам вистачить класичного градієнтного спуску. Далі ми побачимо, що цей алгоритм досить простий у реалізації за допомогою бібліотеки, яку ми розглядаємо. Нехай потрібно навчити нейронну мережу, що складається з одного нейрона. \n",
    "\n",
    "Фактично ми маємо лінійну модель виду:\n",
    "$$\n",
    "a(x) = w1x1 + w2x2 + ...+ wnxn + b\n",
    "$$\n",
    "Створимо клас для даної моделі:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class LinearModel(tf.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.w * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target_y, predicted_y):\n",
    "    return tf.reduce_mean(tf.square(target_y - predicted_y))\n",
    "\n",
    "\n",
    "def train(model, x, y, learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(y, model(x))\n",
    "        dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "        model.w.assign_sub(learning_rate * dw)\n",
    "        model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оновлення ваг ми використовуємо метод `assign_sub`, який веде себе як оператор `-=`. Функція `train` виконають лише одну ітерацію навчання, якої явно недостатньо. Тому нам знадобиться ще одна функція, у якій ми будемо безпосередньо тренувати модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, x, y):\n",
    "    for epoch in range(10):\n",
    "        train(model, x, y, learning_rate=0.1)\n",
    "        current_loss = loss(y, model(x))\n",
    "        print(f\"loss: {current_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того щоб протестувати навчання моделі згенеруємо тестові дані:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_W = 3.0\n",
    "TRUE_B = 2.0\n",
    "\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "x = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "y = x * TRUE_W + TRUE_B + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер зберемо все разом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.9825215339660645\n",
      "loss: 4.1342597007751465\n",
      "loss: 2.9626123905181885\n",
      "loss: 2.2198054790496826\n",
      "loss: 1.7488261461257935\n",
      "loss: 1.4501665830612183\n",
      "loss: 1.2607570886611938\n",
      "loss: 1.1406190395355225\n",
      "loss: 1.0644086599349976\n",
      "loss: 1.0160578489303589\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearModel()\n",
    "training_loop(linear_model, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
